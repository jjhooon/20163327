{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 모듈 import\n",
    "import cv2 \n",
    "import mediapipe as mp\n",
    "import math\n",
    "import numpy as np \n",
    "import os \n",
    "import time\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mediapipe의 face mesh 모델 가져오기\n",
    "# face mesh 속성 설정\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(\n",
    "    min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "# Mediapipe의 face detection 모델 가져오기\n",
    "# face detection 속성 설정\n",
    "mp_face_detection = mp.solutions.face_detection\n",
    "face_detection = mp_face_detection.FaceDetection(\n",
    "    min_detection_confidence = 0.5)\n",
    "\n",
    "# Drawing 스펙 설정\n",
    "mp_drawing = mp.solutions.drawing_utils \n",
    "drawing_spec = mp_drawing.DrawingSpec(color = (255,255,0), thickness=1, circle_radius=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 눈과 입의 특징을 추출하는 함수 정의\n",
    "\n",
    "def distance(p1, p2): # 두 point간 거리 구하는 함수, Euclidean distance\n",
    "    return (((p1[:2] - p2[:2])**2).sum())**0.5\n",
    "\n",
    "def eye_aspect_ratio(landmarks, eye): # 눈 길이(horizontal line)와 눈 너비(vertical line)의 비\n",
    "    N1 = distance(landmarks[eye[1][0]], landmarks[eye[1][1]])\n",
    "    N2 = distance(landmarks[eye[2][0]], landmarks[eye[2][1]])\n",
    "    N3 = distance(landmarks[eye[3][0]], landmarks[eye[3][1]])\n",
    "    D = distance(landmarks[eye[0][0]], landmarks[eye[0][1]])\n",
    "    return (N1 + N2 + N3) / (3 * D)\n",
    "\n",
    "def eye_feature(landmarks): # 양쪽 눈의 eye_aspect_ratio\n",
    "    return (eye_aspect_ratio(landmarks, left_eye) + eye_aspect_ratio(landmarks, right_eye))/2\n",
    "\n",
    "def mouth_feature(landmarks):# eye_aspect_ratio와 마찬가지로 입의 길이와 입 너비의 비를 구하는 함수\n",
    "    N1 = distance(landmarks[mouth[1][0]], landmarks[mouth[1][1]])\n",
    "    N2 = distance(landmarks[mouth[2][0]], landmarks[mouth[2][1]])\n",
    "    N3 = distance(landmarks[mouth[3][0]], landmarks[mouth[3][1]])\n",
    "    D = distance(landmarks[mouth[0][0]], landmarks[mouth[0][1]])\n",
    "    return (N1 + N2 + N3)/(3*D)\n",
    "\n",
    "# FaceMesh mediapipe model의 face landmark를 이용하여 pupil circularity(동공 원형도) 계산\n",
    "def pupil_circularity(landmarks, eye):\n",
    "    perimeter = distance(landmarks[eye[0][0]], landmarks[eye[1][0]]) + \\\n",
    "            distance(landmarks[eye[1][0]], landmarks[eye[2][0]]) + \\\n",
    "            distance(landmarks[eye[2][0]], landmarks[eye[3][0]]) + \\\n",
    "            distance(landmarks[eye[3][0]], landmarks[eye[0][1]]) + \\\n",
    "            distance(landmarks[eye[0][1]], landmarks[eye[3][1]]) + \\\n",
    "            distance(landmarks[eye[3][1]], landmarks[eye[2][1]]) + \\\n",
    "            distance(landmarks[eye[2][1]], landmarks[eye[1][1]]) + \\\n",
    "            distance(landmarks[eye[1][1]], landmarks[eye[0][0]])\n",
    "    area = math.pi * ((distance(landmarks[eye[1][0]], landmarks[eye[3][1]]) * 0.5) ** 2)\n",
    "    return (4*math.pi*area)/(perimeter**2)\n",
    "\n",
    "# 양쪽 눈의 pupil_circularity를 더하여 pupil feature 구하기\n",
    "def pupil_feature(landmarks): \n",
    "    return (pupil_circularity(landmarks, left_eye) + pupil_circularity(landmarks, right_eye))/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image를 받아 eye, mouth, pupil, eye와 mouth의 합, 총 4가지 feature와 drawing image까지 5가지 return하는 함수\n",
    "def run_face_mp(image):\n",
    "    \n",
    "    # image를 수평축을 기준으로 뒤집고 BGR image를 RGB image로 변환한다.\n",
    "    image = cv2.cvtColor(cv2.flip(image, 1), cv2.COLOR_BGR2RGB)\n",
    "    image.flags.writeable = False\n",
    "    \n",
    "    # face mesh와 face detection process 진행\n",
    "    results = face_mesh.process(image)\n",
    "    results2 = face_detection.process(image)\n",
    "    \n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "    # Image에 face detection 박스 표시\n",
    "    # 얼굴 검출에 따라 적절한 문구 출력\n",
    "    if results2.detections:\n",
    "        for detection in results2.detections:\n",
    "            mp_drawing.draw_detection(image, detection)\n",
    "        cv2.putText(image,'There is Driver',(0,30),cv2.FONT_HERSHEY_SIMPLEX,1,(0,255,0),3,cv2.LINE_4,False)\n",
    "    else:\n",
    "        cv2.putText(image,'There is no Driver',(0,30),cv2.FONT_HERSHEY_SIMPLEX,1,(0,0,255),3,cv2.LINE_4,False)\n",
    "    \n",
    "    # image의 face mesh landmark를 이용하여 feature 값을 추출하고, face mesh를 그린다.\n",
    "    if results.multi_face_landmarks:\n",
    "        landmarks_positions = []\n",
    "        # 이미지에 얼굴만 있다고 가정한다.\n",
    "        for _, data_point in enumerate(results.multi_face_landmarks[0].landmark):\n",
    "            landmarks_positions.append([data_point.x, data_point.y, data_point.z]) # normalize된 ladmark position 추가\n",
    "        landmarks_positions = np.array(landmarks_positions)\n",
    "        landmarks_positions[:, 0] *= image.shape[1]\n",
    "        landmarks_positions[:, 1] *= image.shape[0]\n",
    "\n",
    "        # Imgage에 Face mesh 그리기\n",
    "        for face_landmarks in results.multi_face_landmarks:\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    image=image,\n",
    "                    landmark_list=face_landmarks,\n",
    "                    connections=mp_face_mesh.FACE_CONNECTIONS,\n",
    "                    landmark_drawing_spec=drawing_spec,\n",
    "                    connection_drawing_spec=drawing_spec)\n",
    "                \n",
    "        # 위에서 정의했던 함수들을 이용하여 feature를 추출한다.\n",
    "        ear = eye_feature(landmarks_positions)\n",
    "        mar = mouth_feature(landmarks_positions)\n",
    "        puc = pupil_feature(landmarks_positions) # 눈뿐만 아니라 입도 함께 고려하는 puc 정의\n",
    "        moe = mar/ear\n",
    "    else:\n",
    "        ear = -1000\n",
    "        mar = -1000\n",
    "        puc = -1000\n",
    "        moe = -1000\n",
    "\n",
    "    return ear, mar, puc, moe, image\n",
    "\n",
    "# input data로 20 frame의 feature list를 받는다.\n",
    "# 이를 바탕으로 운전자의 상태(정상, 졸음운전) 판단한다.\n",
    "def get_classification(input_data):\n",
    "    model_input = []\n",
    "    model_input.append(input_data[:5])\n",
    "    model_input.append(input_data[3:8])\n",
    "    model_input.append(input_data[6:11])\n",
    "    model_input.append(input_data[9:14])\n",
    "    model_input.append(input_data[12:17])\n",
    "    model_input.append(input_data[15:])\n",
    "    model_input = torch.FloatTensor(np.array(model_input))\n",
    "    preds = torch.sigmoid(model(model_input)).gt(0.5).int().data.numpy()\n",
    "    return int(preds.sum() >= 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 중립 position(기준점)에서 feature를 얻기 위하여 calibrate함수를 정의한다.\n",
    "# default frame은 25로 설정하고, eye, mouth, pupil, eye+mouth 총 4가지의 normalization feature를 return 한다.\n",
    "def calibrate(calib_frame_count=25):\n",
    "    \n",
    "    # Normalize feature value를 담기 위한 빈 리스트 선언\n",
    "    ears = []\n",
    "    mars = []\n",
    "    pucs = []\n",
    "    moes = []\n",
    "    \n",
    "    # 웹캡의 frame 캡처하고, frame 인식이 안될때는 오류 문구 출력\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    while cap.isOpened():\n",
    "        success, image = cap.read()\n",
    "        if not success:\n",
    "            print(\"Ignoring empty camera frame.\")\n",
    "            continue\n",
    "            \n",
    "        # 앞서 만들었던 run_face_mp 함수를 이용하여 feature value를 얻고, 이를 기준값으로 삼는다.\n",
    "        ear, mar,puc, moe, image = run_face_mp(image)\n",
    "        if ear != -1000:\n",
    "            ears.append(ear)\n",
    "            mars.append(mar)\n",
    "            pucs.append(puc)\n",
    "            moes.append(moe)\n",
    "            \n",
    "        # 화면에 Calibration 문구 출력\n",
    "        cv2.putText(image, \"Calibration\", (0,70), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (255,0,0), 3, cv2.LINE_4, False)\n",
    "        cv2.imshow('Calibration', image)\n",
    "        if cv2.waitKey(5) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "        if len(ears) >= calib_frame_count: # ear feature의 크기가 25보다 크면 중지하고, 운전자 상태 모니터링을 시작한다.\n",
    "            break\n",
    "\n",
    "    cv2.destroyAllWindows()\n",
    "    cap.release()\n",
    "    \n",
    "    # 추출한 각 feature를 행렬 형태로 변환하고, normalize하여 리스트 형태로 반환한다.\n",
    "    ears = np.array(ears)\n",
    "    mars = np.array(mars)\n",
    "    pucs = np.array(pucs)\n",
    "    moes = np.array(moes)\n",
    "    return [ears.mean(), ears.std()], [mars.mean(), mars.std()], [pucs.mean(), pucs.std()], [moes.mean(), moes.std()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 운전자의 상태를 모니터링하는 infer 함수 정의\n",
    "# calibration을 통해 얻은 normalization feature를 인자로 받는다.\n",
    "def infer(ears_norm, mars_norm, pucs_norm, moes_norm):\n",
    "    ear_main = 0\n",
    "    mar_main = 0\n",
    "    puc_main = 0\n",
    "    moe_main = 0\n",
    "    decay = 0.9 # feature 값의 noise를 부드럽게 하기 위하여 decay 사용\n",
    "\n",
    "    label = None\n",
    "    \n",
    "    # eye, mar, puc, moe feature 리스트를 추가하기 위한 input_data 리스트 선언\n",
    "    input_data = []\n",
    "    frame_before_run = 0\n",
    "    \n",
    "    # 웹캡의 frame 캡처하고, frame 인식이 안될때는 오류 문구 출력\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    while cap.isOpened():\n",
    "        success, image = cap.read()\n",
    "        if not success:\n",
    "            print(\"Ignoring empty camera frame.\")\n",
    "            continue\n",
    "        \n",
    "        # 실시간으로 얻은 각각의 feature와 Calibration 모드를 통해 얻은 기준과 비교한다.\n",
    "        ear, mar, puc, moe, image = run_face_mp(image)\n",
    "        if ear != -1000:\n",
    "            ear = (ear - ears_norm[0])/ears_norm[1]\n",
    "            mar = (mar - mars_norm[0])/mars_norm[1]\n",
    "            puc = (puc - pucs_norm[0])/pucs_norm[1]\n",
    "            moe = (moe - moes_norm[0])/moes_norm[1]\n",
    "            if ear_main == -1000:\n",
    "                ear_main = ear\n",
    "                mar_main = mar\n",
    "                puc_main = puc\n",
    "                moe_main = moe\n",
    "            else:\n",
    "                ear_main = ear_main*decay + (1-decay)*ear\n",
    "                mar_main = mar_main*decay + (1-decay)*mar\n",
    "                puc_main = puc_main*decay + (1-decay)*puc\n",
    "                moe_main = moe_main*decay + (1-decay)*moe\n",
    "        else:\n",
    "            ear_main = -1000\n",
    "            mar_main = -1000\n",
    "            puc_main = -1000\n",
    "            moe_main = -1000\n",
    "        \n",
    "        if len(input_data) == 20:\n",
    "            input_data.pop(0)\n",
    "        input_data.append([ear_main, mar_main, puc_main, moe_main])\n",
    "        \n",
    "        # 앞서 정의한 get_classfication 함수를 이용하여 운전자의 상태에 맞게 label 출력\n",
    "        frame_before_run += 1\n",
    "        if frame_before_run >= 15 and len(input_data) == 20:\n",
    "            frame_before_run = 0\n",
    "            label = get_classification(input_data)\n",
    "            print ('got label ', label)\n",
    "        \n",
    "        # label이 0이면 정상 상태, label이 1이면 졸음 운전\n",
    "        if label is not None:\n",
    "            if label == 0:\n",
    "                color = (0, 255, 0)\n",
    "            else:\n",
    "                color = (0, 0, 255)\n",
    "            cv2.putText(image, \"%s\" %(states[label]), (0,70),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, color, 3, cv2.LINE_4, False)\n",
    "            \n",
    "        # window 창으로 결과 출력\n",
    "        cv2.imshow('Driver Status Monitoring', image)\n",
    "        if cv2.waitKey(5) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "    \n",
    "    cv2.destroyAllWindows()\n",
    "    cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting calibration. Please be in neutral state\n",
      "Start monitoring\n",
      "got label  0\n",
      "got label  0\n",
      "got label  0\n",
      "got label  0\n",
      "got label  1\n",
      "got label  1\n",
      "got label  1\n",
      "got label  0\n",
      "got label  0\n",
      "got label  0\n",
      "got label  1\n",
      "got label  0\n",
      "got label  0\n",
      "got label  0\n",
      "got label  1\n",
      "got label  1\n",
      "got label  1\n",
      "got label  1\n"
     ]
    }
   ],
   "source": [
    "right_eye = [[33, 133], [160, 144], [159, 145], [158, 153]] # right eye landmark 좌표\n",
    "left_eye = [[263, 362], [387, 373], [386, 374], [385, 380]] # left eye landmark 좌표\n",
    "mouth = [[61, 291], [39, 181], [0, 17], [269, 405]] # mouth landmark 좌표\n",
    "states = ['Nice driving', 'Drowsy. Wake up!!'] # 운전자의 상태 label\n",
    "\n",
    "# LSTM 모델을 이용하여 운전자 상태 예측\n",
    "# 5장의 frame을 거쳐 예측을 하고, 예측 상태를 return한다.\n",
    "model_lstm_path = 'Downloads\\clf_lstm_jit6.pth'\n",
    "model = torch.jit.load(model_lstm_path)\n",
    "model.eval()\n",
    "\n",
    "# calibrate 함수를 통해 normalize feature 획득\n",
    "print ('Starting calibration. Please be in neutral state')\n",
    "time.sleep(1)\n",
    "ears_norm, mars_norm, pucs_norm, moes_norm = calibrate()\n",
    "\n",
    "# normalize feature를 이용하여 운전자 상태 모니터링\n",
    "print ('Start monitoring')\n",
    "time.sleep(1)\n",
    "infer(ears_norm, mars_norm, pucs_norm, moes_norm)\n",
    "\n",
    "# mediapipe의 face mesh와 face detection을 끝낸다.\n",
    "face_mesh.close()\n",
    "face_detection.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
